from pymongo import MongoClient
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import List, Dict
import datetime
import os
from settings import settings


class MongoDBAtlasHandler:
    def __init__(self, use_chinese_collection: bool = False):
        """åˆå§‹åŒ–è¿æ¥ï¼Œæ”¯æŒåˆ‡æ¢å‘é‡é›†åˆ"""
        try:
            self.client = MongoClient(settings.mongo_uri)
            self.client.admin.command('ping')
            print(f"âœ… æˆåŠŸè¿æ¥MongoDB Atlasï¼š{settings.mongo_uri.split('@')[-1]}")
        except Exception as e:
            raise ConnectionError(f"âŒ MongoDBè¿æ¥å¤±è´¥ï¼š{str(e)}") from e

        self.db = self.client[settings.mongo_db_name]
        self.doc_collection = self.db[settings.mongo_collection_name]
        # æ ¹æ®æ ‡å¿—é€‰æ‹©å‘é‡é›†åˆ
        if use_chinese_collection:
            self.vector_collection = self.db[settings.mongo_vector_collection_chinese]
        else:
            self.vector_collection = self.db[settings.mongo_vector_collection]

    def _create_vector_index(self):
        """ç§»é™¤å‘é‡ç´¢å¼•åˆ›å»ºé€»è¾‘ï¼Œåœ¨æœ¬åœ°è¿›è¡Œå‘é‡æ£€ç´¢"""
        pass

    def store_document(self, document: Document) -> str:
        """å­˜å‚¨å®Œæ•´æ–‡æ¡£åˆ°Atlas"""
        doc_data = {
            "page_content": document.page_content,
            "metadata": document.metadata,
            "created_at": datetime.datetime.utcnow()
        }
        insert_result = self.doc_collection.insert_one(doc_data)
        return str(insert_result.inserted_id)

    def store_embedding(self, doc_id: str, embedding: List[float], chunk_text: str, chunk_metadata: Dict):
        """å­˜å‚¨æ–‡æ¡£ç‰‡æ®µå’Œå‘é‡åˆ°Atlas"""
        embedding_data = {
            "doc_id": doc_id,
            "embedding": embedding,  # 1536ç»´å‘é‡
            "chunk_text": chunk_text,
            "metadata": chunk_metadata,
            "created_at": datetime.datetime.utcnow()
        }
        self.vector_collection.insert_one(embedding_data)

    def retrieve_similar_vectors(self, query_embedding: List[float], top_k: int = None) -> List[Dict]:
        """ä½¿ç”¨æœ¬åœ°ä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œå‘é‡æ£€ç´¢"""
        top_k = top_k or settings.retrieval_top_k
        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£çš„å‘é‡
            all_docs = list(
                self.vector_collection.find({}, {'embedding': 1, 'chunk_text': 1, 'metadata': 1, 'doc_id': 1}))

            if not all_docs:
                return []

            # æ„å»ºå‘é‡çŸ©é˜µ
            embeddings_matrix = np.array([doc['embedding'] for doc in all_docs])
            query_embedding = np.array(query_embedding).reshape(1, -1)

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarities = cosine_similarity(query_embedding, embeddings_matrix)[0]

            # è·å–top_kä¸ªæœ€ç›¸ä¼¼çš„æ–‡æ¡£ç´¢å¼•
            top_indices = np.argsort(similarities)[-top_k:][::-1]

            # æ„å»ºç»“æœ
            results = []
            for idx in top_indices:
                doc = all_docs[idx]
                similarity_score = float(similarities[idx])
                results.append({
                    **doc,
                    "similarity_score": similarity_score,  # ç¡®ä¿è¿™ä¸ªå­—æ®µå­˜åœ¨
                    "chunk_text": doc["chunk_text"],
                    "metadata": {**doc["metadata"], "similarity_score": similarity_score}  # å­˜å…¥metadata
                })
            return results

        except Exception as e:
            print(f"âŒ å‘é‡æ£€ç´¢å¤±è´¥ï¼š{str(e)}")
            return []

    def get_existing_chunks(self, doc_id: str) -> List[Dict]:
        """ä»æ•°æ®åº“è¯»å–æŒ‡å®šæ–‡æ¡£çš„å·²æœ‰chunks"""
        return list(self.vector_collection.find(
            {"doc_id": doc_id},
            {"chunk_text": 1, "metadata": 1, "embedding": 1, "_id": 0}
        ))

    def close_connection(self):
        """å…³é—­Atlasè¿æ¥"""
        self.client.close()
        print("ğŸ”Œ MongoDB Atlasè¿æ¥å·²å…³é—­")


class DocumentProcessor:
    def __init__(self, mongo_handler: MongoDBAtlasHandler, model_manager):
        """æ–‡æ¡£å¤„ç†å™¨ï¼ˆæ”¯æŒå¤ç”¨å·²æœ‰chunksï¼‰"""
        self.mongo_handler = mongo_handler
        self.model_manager = model_manager
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.chunk_size,
            chunk_overlap=settings.chunk_overlap,
            separators=["\n\n", "\n"]
        )

    def load_md_document(self, file_path: str) -> Document:
        """åŠ è½½æœ¬åœ°Markdownæ–‡æ¡£"""
        if not os.path.isabs(file_path):
            file_path = os.path.abspath(os.path.join(os.path.dirname(__file__), file_path))

        if not os.path.exists(file_path):
            raise FileNotFoundError(f"âŒ æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
        except UnicodeDecodeError:
            raise ValueError(f"âŒ æ–‡ä»¶ç¼–ç é”™è¯¯ï¼Œè¯·ä½¿ç”¨UTF-8ï¼š{file_path}")

        return Document(
            page_content=content,
            metadata={
                "source": file_path,
                "format": "markdown",
                "loaded_at": str(datetime.datetime.utcnow())
            }
        )

    def process_and_store(self, document: Document, force_reprocess: bool = False) -> str:
        """å¤„ç†å¹¶å­˜å‚¨æ–‡æ¡£åˆ°Atlasï¼Œæ”¯æŒå¼ºåˆ¶é‡æ–°å¤„ç†"""
        # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨
        existing_doc = self.mongo_handler.doc_collection.find_one({
            "metadata.source": document.metadata["source"]
        })

        if existing_doc and not force_reprocess:
            doc_id = str(existing_doc["_id"])
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰chunks
            existing_chunks = self.mongo_handler.get_existing_chunks(doc_id)
            if existing_chunks:
                print(f"ğŸ“„ æ–‡æ¡£åŠåˆ†ç‰‡å·²å­˜åœ¨äºAtlasï¼ŒIDï¼š{doc_id}ï¼Œè·³è¿‡å¤„ç†")
                return doc_id

        # æ–°æ–‡æ¡£æˆ–å¼ºåˆ¶é‡æ–°å¤„ç†æ—¶æ‰§è¡Œåˆ†ç‰‡
        doc_id = self.mongo_handler.store_document(document)
        print(f"ğŸ“„ æ–‡æ¡£å·²å­˜å‚¨åˆ°Atlasï¼ŒIDï¼š{doc_id}")

        chunks = self.text_splitter.split_documents([document])
        print(f"âœ‚ï¸ åˆ†å‰²ä¸º {len(chunks)} ä¸ªç‰‡æ®µ")

        for chunk_idx, chunk in enumerate(chunks):
            chunk_embedding = self.model_manager.get_embedding(chunk.page_content)
            chunk_metadata = {
                **chunk.metadata,
                "chunk_idx": chunk_idx,
                "total_chunks": len(chunks)
            }
            self.mongo_handler.store_embedding(
                doc_id=doc_id,
                embedding=chunk_embedding,
                chunk_text=chunk.page_content,
                chunk_metadata=chunk_metadata
            )

        return doc_id

    def init_knowledge_base(self, force_reprocess: bool = False):
        """åˆå§‹åŒ–çŸ¥è¯†åº“ï¼Œæ”¯æŒå¼ºåˆ¶é‡æ–°å¤„ç†"""
        print(f"\nğŸ”§ åˆå§‹åŒ–çŸ¥è¯†åº“ï¼š{settings.knowledge_base_path}")

        existing_docs = self.mongo_handler.doc_collection.count_documents({
            "metadata.source": settings.knowledge_base_path
        })

        if existing_docs > 0 and not force_reprocess:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰chunks
            doc_id = str(self.mongo_handler.doc_collection.find_one(
                {"metadata.source": settings.knowledge_base_path})["_id"])
            existing_chunks = self.mongo_handler.get_existing_chunks(doc_id)
            if existing_chunks:
                print(f"âœ… çŸ¥è¯†åº“åŠåˆ†ç‰‡å·²å­˜åœ¨äºAtlasï¼Œè·³è¿‡åŠ è½½")
                return

        try:
            md_document = self.load_md_document(settings.knowledge_base_path)
            self.process_and_store(md_document, force_reprocess=force_reprocess)
            print("âœ… çŸ¥è¯†åº“å·²ä¸Šä¼ åˆ°Atlasï¼")
        except Exception as e:
            print(f"âŒ çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}")
            raise

    def init_chinese_knowledge_base(self, force_reprocess: bool = False):
        """åˆå§‹åŒ–ä¸­æ–‡çŸ¥è¯†åº“"""
        print(f"\nğŸ”§ åˆå§‹åŒ–ä¸­æ–‡çŸ¥è¯†åº“ï¼š{settings.knowledge_base_path_chinese}")

        existing_docs = self.mongo_handler.doc_collection.count_documents({
            "metadata.source": settings.knowledge_base_path_chinese
        })

        if existing_docs > 0 and not force_reprocess:
            doc_id = str(self.mongo_handler.doc_collection.find_one(
                {"metadata.source": settings.knowledge_base_path_chinese})["_id"])
            existing_chunks = self.mongo_handler.get_existing_chunks(doc_id)
            if existing_chunks:
                print(f"âœ… ä¸­æ–‡çŸ¥è¯†åº“åŠåˆ†ç‰‡å·²å­˜åœ¨ï¼Œè·³è¿‡åŠ è½½")
                return

        try:
            md_document = self.load_md_document(settings.knowledge_base_path_chinese)
            self.process_and_store(md_document, force_reprocess=force_reprocess)
            print("âœ… ä¸­æ–‡çŸ¥è¯†åº“å·²ä¸Šä¼ åˆ°Atlasï¼")
        except Exception as e:
            print(f"âŒ ä¸­æ–‡çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}")
            raise